{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Datathon FIAP - Passos M√°gicos\n",
    "## Modelo Preditivo de Risco de Defasagem\n",
    "\n",
    "Este notebook desenvolve e compara m√∫ltiplos modelos de Machine Learning para identificar alunos em risco de defasagem educacional.\n",
    "\n",
    "**Objetivo:** Criar um modelo preditivo que identifique padr√µes nos indicadores de desempenho e vari√°veis contextuais que permitam prever alunos em risco de defasagem escolar.\n",
    "\n",
    "**Classifica√ß√£o de Risco:**\n",
    "- **Sem Risco**: Aluno em fase adequada ou adiantado (D ‚â• 0)\n",
    "- **Com Risco**: Aluno atrasado em rela√ß√£o √† fase ideal (D < 0)\n",
    "\n",
    "**Features utilizadas:**\n",
    "- Indicadores PEDE: IDA, IEG, IAA, IPS, IPV\n",
    "- Notas: Matem√°tica, Portugu√™s, Ingl√™s\n",
    "- Contextuais: Idade, Ano de Ingresso, G√™nero, Institui√ß√£o de Ensino\n",
    "\n",
    "**Autor:** Leandro Leme Crespo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar dados do GitHub (executar apenas no Google Colab)\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('streamlit', exist_ok=True)\n",
    "\n",
    "url = 'https://github.com/LeandroCrespo/datathon-passos-magicos/raw/main/data/BASE_DE_DADOS_PEDE_2024_DATATHON.xlsx'\n",
    "filename = 'data/BASE_DE_DADOS_PEDE_2024_DATATHON.xlsx'\n",
    "\n",
    "print('üì• Baixando dados do GitHub...')\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print('‚úÖ Dados baixados com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "!pip install openpyxl scikit-learn imbalanced-learn xgboost -q\n",
    "print('‚úÖ Bibliotecas instaladas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ Bibliotecas importadas com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "CAMINHO_ARQUIVO = 'data/BASE_DE_DADOS_PEDE_2024_DATATHON.xlsx'\n",
    "\n",
    "xlsx = pd.ExcelFile(CAMINHO_ARQUIVO)\n",
    "print(f'üìã Planilhas dispon√≠veis: {xlsx.sheet_names}')\n",
    "\n",
    "all_data = []\n",
    "for sheet in xlsx.sheet_names:\n",
    "    df_year = pd.read_excel(xlsx, sheet_name=sheet)\n",
    "    df_year.columns = [c.upper() for c in df_year.columns]\n",
    "    \n",
    "    # Padronizar colunas\n",
    "    if 'DEFAS' in df_year.columns and 'DEFASAGEM' not in df_year.columns:\n",
    "        df_year = df_year.rename(columns={'DEFAS': 'DEFASAGEM'})\n",
    "    if 'MATEM' in df_year.columns:\n",
    "        df_year = df_year.rename(columns={'MATEM': 'MAT'})\n",
    "    if 'PORTUG' in df_year.columns:\n",
    "        df_year = df_year.rename(columns={'PORTUG': 'POR'})\n",
    "    if 'INGL√äS' in df_year.columns:\n",
    "        df_year = df_year.rename(columns={'INGL√äS': 'ING'})\n",
    "    if 'G√äNERO' in df_year.columns:\n",
    "        df_year['G√äNERO'] = df_year['G√äNERO'].replace({'Menina': 'Feminino', 'Menino': 'Masculino'})\n",
    "    \n",
    "    if 'DEFASAGEM' in df_year.columns:\n",
    "        df_year['ANO_PEDE'] = sheet\n",
    "        all_data.append(df_year)\n",
    "        print(f'   {sheet}: {len(df_year)} registros')\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "print(f'\\nüìä Total de registros: {len(df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features\n",
    "# NOTA: Exclu√≠mos INDE e IAN para evitar data leakage\n",
    "features_numericas = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV', 'IDADE', 'ANO INGRESSO', 'MAT', 'POR', 'ING']\n",
    "features_categoricas = ['G√äNERO', 'INSTITUI√á√ÉO DE ENSINO']\n",
    "\n",
    "# Converter num√©ricas\n",
    "for col in features_numericas:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Converter categ√≥ricas\n",
    "le_dict = {}\n",
    "for col in features_categoricas:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Desconhecido')\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_ENC'] = le.fit_transform(df[col].astype(str))\n",
    "        le_dict[col] = le\n",
    "        print(f'{col}: {dict(zip(le.classes_, range(len(le.classes_))))}')\n",
    "\n",
    "print('\\n‚úÖ Features preparadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defini√ß√£o da Vari√°vel Alvo\n",
    "\n",
    "**D = Fase Efetiva - Fase Ideal**\n",
    "\n",
    "| Defasagem (D) | Classifica√ß√£o |\n",
    "|---------------|---------------|\n",
    "| D ‚â• 0 | Sem Risco |\n",
    "| D < 0 | Com Risco |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar vari√°vel alvo\n",
    "df['DEFASAGEM'] = pd.to_numeric(df['DEFASAGEM'], errors='coerce')\n",
    "\n",
    "def classificar_risco(d):\n",
    "    if pd.isna(d): return None\n",
    "    if d >= 0: return 0  # Sem Risco\n",
    "    else: return 1  # Com Risco\n",
    "\n",
    "df['CLASSE_RISCO'] = df['DEFASAGEM'].apply(classificar_risco)\n",
    "\n",
    "# Selecionar features dispon√≠veis\n",
    "features_final = []\n",
    "for col in features_numericas:\n",
    "    if col in df.columns:\n",
    "        features_final.append(col)\n",
    "for col in features_categoricas:\n",
    "    if col + '_ENC' in df.columns:\n",
    "        features_final.append(col + '_ENC')\n",
    "\n",
    "print(f'Features utilizadas ({len(features_final)}): {features_final}')\n",
    "\n",
    "# Remover linhas com valores nulos\n",
    "df_model = df.dropna(subset=['CLASSE_RISCO'] + features_final)\n",
    "df_model['CLASSE_RISCO'] = df_model['CLASSE_RISCO'].astype(int)\n",
    "\n",
    "print(f'\\nRegistros v√°lidos: {len(df_model):,}')\n",
    "print(f'Sem Risco (0): {(df_model[\"CLASSE_RISCO\"] == 0).sum()} ({(df_model[\"CLASSE_RISCO\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'Com Risco (1): {(df_model[\"CLASSE_RISCO\"] == 1).sum()} ({(df_model[\"CLASSE_RISCO\"] == 1).mean()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribui√ß√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "cores = ['#2ecc71', '#e74c3c']\n",
    "labels = ['Sem Risco', 'Com Risco']\n",
    "valores = df_model['CLASSE_RISCO'].value_counts().sort_index()\n",
    "\n",
    "bars = axes[0].bar(labels, valores.values, color=cores, edgecolor='black')\n",
    "axes[0].set_title('Distribui√ß√£o das Classes de Risco', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Quantidade de Alunos')\n",
    "for bar, val in zip(bars, valores.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, f'{val}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].pie(valores.values, labels=labels, colors=cores, autopct='%1.1f%%', startangle=90, explode=(0, 0.05))\n",
    "axes[1].set_title('Propor√ß√£o das Classes de Risco', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribuicao_risco.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise dos Padr√µes de Risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©dia dos indicadores por classe\n",
    "indicadores = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV']\n",
    "media_por_classe = df_model.groupby('CLASSE_RISCO')[indicadores].mean()\n",
    "media_por_classe.index = ['Sem Risco', 'Com Risco']\n",
    "\n",
    "print('üìä M√©dia dos Indicadores por Classe de Risco:')\n",
    "print(media_por_classe.round(2))\n",
    "\n",
    "print('\\nüìä Diferen√ßa (Sem Risco - Com Risco):')\n",
    "diff = media_por_classe.loc['Sem Risco'] - media_por_classe.loc['Com Risco']\n",
    "for feat in diff.sort_values(ascending=False).index:\n",
    "    print(f'   {feat}: +{diff[feat]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Idade\n",
    "print('üìä An√°lise de Idade:')\n",
    "print(f'   Sem Risco - M√©dia: {df_model[df_model[\"CLASSE_RISCO\"]==0][\"IDADE\"].mean():.1f} anos')\n",
    "print(f'   Com Risco - M√©dia: {df_model[df_model[\"CLASSE_RISCO\"]==1][\"IDADE\"].mean():.1f} anos')\n",
    "print('\\n‚Üí Alunos mais velhos t√™m MAIOR risco de defasagem')\n",
    "print('  (A idade funciona como multiplicador - ac√∫mulo de comportamentos ao longo do tempo)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o dos Dados para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X = df_model[features_final].values\n",
    "y = df_model['CLASSE_RISCO'].values\n",
    "\n",
    "print(f'Shape X: {X.shape}')\n",
    "print(f'Shape y: {y.shape}')\n",
    "\n",
    "# Split treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f'\\nTreino: {len(X_train)} amostras')\n",
    "print(f'Teste: {len(X_test)} amostras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('‚úÖ Dados normalizados')\n",
    "\n",
    "# SMOTE para balancear\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f'\\nAp√≥s SMOTE:')\n",
    "print(f'   Treino original: {len(X_train_scaled)} amostras')\n",
    "print(f'   Treino balanceado: {len(X_train_balanced)} amostras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compara√ß√£o de Modelos de Machine Learning\n",
    "\n",
    "Vamos comparar **10 algoritmos diferentes** para encontrar o melhor modelo:\n",
    "\n",
    "| Categoria | Modelos |\n",
    "|-----------|--------|\n",
    "| **Lineares** | Logistic Regression |\n",
    "| **Baseados em Dist√¢ncia** | K-Nearest Neighbors (KNN), SVM |\n",
    "| **Baseados em √Årvore** | Decision Tree, Random Forest, Gradient Boosting, XGBoost, AdaBoost |\n",
    "| **Redes Neurais** | MLP (Multi-Layer Perceptron) |\n",
    "| **Ensemble** | Voting Classifier |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir todos os modelos a serem testados\n",
    "modelos = {\n",
    "    # Modelos Lineares\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \n",
    "    # Baseados em Dist√¢ncia\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \n",
    "    # Baseados em √Årvore\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=7, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, max_depth=7, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    \n",
    "    # Redes Neurais\n",
    "    'MLP (Rede Neural)': MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "print(f'Total de modelos a testar: {len(modelos)}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar cada modelo\n",
    "print('\\n' + '='*70)\n",
    "print('TREINAMENTO E AVALIA√á√ÉO DOS MODELOS')\n",
    "print('='*70 + '\\n')\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f'Treinando {nome}...')\n",
    "    \n",
    "    # Treinar\n",
    "    modelo.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = modelo.predict(X_test_scaled)\n",
    "    y_proba = modelo.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(modelo, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    resultados.append({\n",
    "        'Modelo': nome,\n",
    "        'Acur√°cia': acc,\n",
    "        'Precis√£o': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'modelo_obj': modelo,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba\n",
    "    })\n",
    "    \n",
    "    print(f'   Acur√°cia: {acc*100:.2f}% | AUC-ROC: {auc*100:.2f}% | CV: {cv_mean*100:.2f}% (¬±{cv_std*100:.2f}%)')\n",
    "\n",
    "print('\\n‚úÖ Todos os modelos treinados!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Ensemble Voting com os 3 melhores modelos\n",
    "print('\\nCriando Ensemble Voting...')\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', modelos['Random Forest']),\n",
    "        ('gb', modelos['Gradient Boosting']),\n",
    "        ('mlp', modelos['MLP (Rede Neural)'])\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred_ens = voting.predict(X_test_scaled)\n",
    "y_proba_ens = voting.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "acc_ens = accuracy_score(y_test, y_pred_ens)\n",
    "prec_ens = precision_score(y_test, y_pred_ens)\n",
    "rec_ens = recall_score(y_test, y_pred_ens)\n",
    "f1_ens = f1_score(y_test, y_pred_ens)\n",
    "auc_ens = roc_auc_score(y_test, y_proba_ens)\n",
    "cv_ens = cross_val_score(voting, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "\n",
    "resultados.append({\n",
    "    'Modelo': 'Ensemble Voting',\n",
    "    'Acur√°cia': acc_ens,\n",
    "    'Precis√£o': prec_ens,\n",
    "    'Recall': rec_ens,\n",
    "    'F1-Score': f1_ens,\n",
    "    'AUC-ROC': auc_ens,\n",
    "    'CV Mean': cv_ens.mean(),\n",
    "    'CV Std': cv_ens.std(),\n",
    "    'modelo_obj': voting,\n",
    "    'y_pred': y_pred_ens,\n",
    "    'y_proba': y_proba_ens\n",
    "})\n",
    "\n",
    "print(f'   Acur√°cia: {acc_ens*100:.2f}% | AUC-ROC: {auc_ens*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ranking dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Ordenar por Acur√°cia\n",
    "df_ranking = df_resultados[['Modelo', 'Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC', 'CV Mean', 'CV Std']].copy()\n",
    "df_ranking = df_ranking.sort_values('Acur√°cia', ascending=False).reset_index(drop=True)\n",
    "df_ranking.index = df_ranking.index + 1  # Ranking come√ßa em 1\n",
    "\n",
    "# Formatar para exibi√ß√£o\n",
    "df_display = df_ranking.copy()\n",
    "for col in ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC', 'CV Mean']:\n",
    "    df_display[col] = df_display[col].apply(lambda x: f'{x*100:.2f}%')\n",
    "df_display['CV Std'] = df_display['CV Std'].apply(lambda x: f'¬±{x*100:.2f}%')\n",
    "\n",
    "print('='*100)\n",
    "print('üìä RANKING DOS MODELOS DE MACHINE LEARNING')\n",
    "print('='*100)\n",
    "print(df_display.to_string())\n",
    "print('\\n')\n",
    "\n",
    "# Destacar o melhor\n",
    "melhor = df_ranking.iloc[0]\n",
    "print(f'üèÜ MELHOR MODELO: {melhor[\"Modelo\"]}')\n",
    "print(f'   Acur√°cia: {melhor[\"Acur√°cia\"]*100:.2f}%')\n",
    "print(f'   AUC-ROC: {melhor[\"AUC-ROC\"]*100:.2f}%')\n",
    "print(f'   F1-Score: {melhor[\"F1-Score\"]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gr√°fico 1: Acur√°cia\n",
    "df_plot = df_ranking[['Modelo', 'Acur√°cia']].sort_values('Acur√°cia')\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.9, len(df_plot)))\n",
    "bars = axes[0].barh(df_plot['Modelo'], df_plot['Acur√°cia']*100, color=colors)\n",
    "axes[0].set_xlabel('Acur√°cia (%)')\n",
    "axes[0].set_title('Compara√ß√£o de Acur√°cia dos Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0, 100)\n",
    "for bar, val in zip(bars, df_plot['Acur√°cia']):\n",
    "    axes[0].text(val*100 + 1, bar.get_y() + bar.get_height()/2, f'{val*100:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# Gr√°fico 2: M√∫ltiplas m√©tricas\n",
    "metricas = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(df_ranking))\n",
    "width = 0.2\n",
    "\n",
    "for i, metrica in enumerate(metricas):\n",
    "    axes[1].bar(x + i*width, df_ranking[metrica]*100, width, label=metrica)\n",
    "\n",
    "axes[1].set_ylabel('Porcentagem (%)')\n",
    "axes[1].set_title('Compara√ß√£o de M√©tricas por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x + width*1.5)\n",
    "axes[1].set_xticklabels(df_ranking['Modelo'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparacao_modelos.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC de todos os modelos\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for resultado in resultados:\n",
    "    fpr, tpr, _ = roc_curve(y_test, resultado['y_proba'])\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{resultado['Modelo']} (AUC={resultado['AUC-ROC']*100:.1f}%)\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Aleat√≥rio')\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curvas ROC - Compara√ß√£o de Modelos', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('curvas_roc.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise do Modelo Escolhido\n",
    "\n",
    "Com base na compara√ß√£o, selecionamos o modelo com melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar melhor modelo\n",
    "idx_melhor = df_resultados['Acur√°cia'].idxmax()\n",
    "melhor_resultado = df_resultados.iloc[idx_melhor]\n",
    "modelo_final = melhor_resultado['modelo_obj']\n",
    "y_pred_final = melhor_resultado['y_pred']\n",
    "y_proba_final = melhor_resultado['y_proba']\n",
    "\n",
    "print(f'üìä Relat√≥rio de Classifica√ß√£o - {melhor_resultado[\"Modelo\"]}')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Sem Risco', 'Com Risco']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confus√£o\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Sem Risco', 'Com Risco'], yticklabels=['Sem Risco', 'Com Risco'])\n",
    "ax.set_xlabel('Predito')\n",
    "ax.set_ylabel('Real')\n",
    "ax.set_title(f'Matriz de Confus√£o - {melhor_resultado[\"Modelo\"]}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('matriz_confusao.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpreta√ß√£o\n",
    "print('\\nüìä Interpreta√ß√£o da Matriz de Confus√£o:')\n",
    "print(f'   Verdadeiros Negativos (Sem Risco correto): {cm[0,0]}')\n",
    "print(f'   Falsos Positivos (Sem Risco predito como Com Risco): {cm[0,1]}')\n",
    "print(f'   Falsos Negativos (Com Risco predito como Sem Risco): {cm[1,0]}')\n",
    "print(f'   Verdadeiros Positivos (Com Risco correto): {cm[1,1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (usando Random Forest para interpretabilidade)\n",
    "rf_model = modelos['Random Forest']\n",
    "\n",
    "# Mapear nomes das features\n",
    "feature_names = features_final.copy()\n",
    "for i, name in enumerate(feature_names):\n",
    "    if '_ENC' in name:\n",
    "        feature_names[i] = name.replace('_ENC', '')\n",
    "\n",
    "df_imp = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Import√¢ncia': rf_model.feature_importances_\n",
    "}).sort_values('Import√¢ncia', ascending=False)\n",
    "\n",
    "print('üìà Import√¢ncia das Features (Random Forest):')\n",
    "print('='*50)\n",
    "for _, row in df_imp.iterrows():\n",
    "    barra = '‚ñà' * int(row['Import√¢ncia'] * 50)\n",
    "    print(f'{row[\"Feature\"]:20}: {row[\"Import√¢ncia\"]*100:5.1f}% {barra}')\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(df_imp)))\n",
    "bars = ax.barh(df_imp['Feature'], df_imp['Import√¢ncia']*100, color=colors)\n",
    "ax.set_xlabel('Import√¢ncia (%)')\n",
    "ax.set_title('Import√¢ncia das Features para Predi√ß√£o de Risco', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, df_imp['Import√¢ncia']):\n",
    "    ax.text(val*100 + 0.5, bar.get_y() + bar.get_height()/2, f'{val*100:.1f}%', va='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('importancia_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Salvar Modelo para Deploy (Streamlit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo, scaler e metadados\n",
    "output_dir = 'streamlit/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Salvar modelo\n",
    "with open(f'{output_dir}modelo_risco_defasagem.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo_final, f)\n",
    "print(f'‚úÖ Modelo salvo')\n",
    "\n",
    "# Salvar scaler\n",
    "with open(f'{output_dir}scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f'‚úÖ Scaler salvo')\n",
    "\n",
    "# Salvar LabelEncoders\n",
    "with open(f'{output_dir}label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(le_dict, f)\n",
    "print(f'‚úÖ LabelEncoders salvos')\n",
    "\n",
    "# Salvar features\n",
    "with open(f'{output_dir}features.txt', 'w') as f:\n",
    "    f.write(','.join(features_final))\n",
    "print(f'‚úÖ Features salvas')\n",
    "\n",
    "# Salvar info do modelo\n",
    "modelo_info = {\n",
    "    'features': features_final,\n",
    "    'features_numericas': [f for f in features_final if '_ENC' not in f],\n",
    "    'features_categoricas': [f.replace('_ENC', '') for f in features_final if '_ENC' in f],\n",
    "    'classes': {0: 'Sem Risco', 1: 'Com Risco'},\n",
    "    'accuracy': float(melhor_resultado['Acur√°cia']),\n",
    "    'auc_roc': float(melhor_resultado['AUC-ROC']),\n",
    "    'modelo_nome': melhor_resultado['Modelo'],\n",
    "    'feature_importance': dict(zip(feature_names, rf_model.feature_importances_.tolist()))\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}modelo_info.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo_info, f)\n",
    "print(f'‚úÖ Info do modelo salva')\n",
    "\n",
    "print(f'\\nüìÅ Arquivos salvos em {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclus√µes\n",
    "\n",
    "### Compara√ß√£o de Modelos\n",
    "\n",
    "Foram testados **10 algoritmos de Machine Learning** diferentes:\n",
    "\n",
    "| Categoria | Modelos Testados |\n",
    "|-----------|------------------|\n",
    "| Lineares | Logistic Regression |\n",
    "| Baseados em Dist√¢ncia | KNN, SVM |\n",
    "| Baseados em √Årvore | Decision Tree, Random Forest, Gradient Boosting, XGBoost, AdaBoost |\n",
    "| Redes Neurais | MLP |\n",
    "| Ensemble | Voting Classifier |\n",
    "\n",
    "### Por que escolhemos o modelo final?\n",
    "\n",
    "O modelo foi escolhido com base em:\n",
    "1. **Maior Acur√°cia** no conjunto de teste\n",
    "2. **Melhor AUC-ROC** (capacidade de distinguir entre classes)\n",
    "3. **Estabilidade** na valida√ß√£o cruzada (baixo desvio padr√£o)\n",
    "\n",
    "### Principais Insights\n",
    "\n",
    "1. **IDADE √© o fator mais importante (~38%)** - Funciona como multiplicador de risco\n",
    "2. **IEG (Engajamento)** √© o indicador PEDE mais relevante (~10%)\n",
    "3. **Notas de Matem√°tica** t√™m maior poder preditivo que outras mat√©rias\n",
    "\n",
    "### Limita√ß√µes\n",
    "\n",
    "- Os indicadores de desempenho atual capturam apenas parte do risco\n",
    "- A defasagem √© um ac√∫mulo hist√≥rico que depende de fatores al√©m do desempenho atual\n",
    "- O modelo deve ser usado como ferramenta de triagem, n√£o como decis√£o final\n",
    "\n",
    "### Recomenda√ß√µes\n",
    "\n",
    "1. **Interven√ß√£o precoce** em alunos jovens com indicadores baixos\n",
    "2. **Aten√ß√£o especial** a alunos mais velhos com desempenho m√©dio/baixo\n",
    "3. **Monitoramento cont√≠nuo** dos indicadores IEG e IPV como sinais de alerta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}