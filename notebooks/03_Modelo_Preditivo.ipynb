{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Datathon FIAP - Passos M√°gicos\n",
    "## Modelo Preditivo de Risco de Defasagem\n",
    "\n",
    "Este notebook desenvolve um modelo de Machine Learning para identificar alunos em risco de defasagem educacional, utilizando os indicadores do PEDE.\n",
    "\n",
    "**Objetivo:** Criar um modelo preditivo que identifique padr√µes nos indicadores que permitam prever alunos em risco antes de queda no desempenho ou aumento da defasagem.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive (executar apenas no Google Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "!pip install openpyxl scikit-learn xgboost imbalanced-learn shap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, auc)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep Learning (Keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print('‚úÖ Bibliotecas importadas com sucesso!')\n",
    "print(f'TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "CAMINHO_ARQUIVO = '/content/drive/MyDrive/Datathon/BASE_DE_DADOS_PEDE_2024_DATATHON.xlsx'\n",
    "\n",
    "xlsx = pd.ExcelFile(CAMINHO_ARQUIVO)\n",
    "df_2022 = pd.read_excel(xlsx, sheet_name='PEDE2022')\n",
    "df_2023 = pd.read_excel(xlsx, sheet_name='PEDE2023')\n",
    "df_2024 = pd.read_excel(xlsx, sheet_name='PEDE2024')\n",
    "\n",
    "print(f'üìä Dados carregados:')\n",
    "print(f'   PEDE 2022: {df_2022.shape[0]:,} alunos')\n",
    "print(f'   PEDE 2023: {df_2023.shape[0]:,} alunos')\n",
    "print(f'   PEDE 2024: {df_2024.shape[0]:,} alunos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para preparar dados\n",
    "def preparar_dados(df, ano):\n",
    "    df_prep = df.copy()\n",
    "    df_prep['ANO'] = ano\n",
    "    \n",
    "    col_map = {}\n",
    "    for col in df_prep.columns:\n",
    "        col_lower = col.lower()\n",
    "        if col_lower == 'iaa': col_map[col] = 'IAA'\n",
    "        elif col_lower == 'ieg' and 'destaque' not in col_lower: col_map[col] = 'IEG'\n",
    "        elif col_lower == 'ips': col_map[col] = 'IPS'\n",
    "        elif col_lower == 'ipp': col_map[col] = 'IPP'\n",
    "        elif col_lower == 'ida' and 'destaque' not in col_lower: col_map[col] = 'IDA'\n",
    "        elif col_lower == 'ipv' and 'destaque' not in col_lower: col_map[col] = 'IPV'\n",
    "        elif col_lower == 'ian': col_map[col] = 'IAN'\n",
    "        elif 'defas' in col_lower: col_map[col] = 'DEFASAGEM'\n",
    "        elif col_lower == 'fase': col_map[col] = 'FASE'\n",
    "        elif col_lower == 'g√™nero' or col_lower == 'genero': col_map[col] = 'GENERO'\n",
    "    \n",
    "    if ano == 2022:\n",
    "        col_map['INDE 22'] = 'INDE'\n",
    "        col_map['Pedra 22'] = 'PEDRA'\n",
    "    elif ano == 2023:\n",
    "        col_map['INDE 2023'] = 'INDE'\n",
    "        col_map['Pedra 2023'] = 'PEDRA'\n",
    "    elif ano == 2024:\n",
    "        col_map['INDE 2024'] = 'INDE'\n",
    "        col_map['Pedra 2024'] = 'PEDRA'\n",
    "    \n",
    "    df_prep = df_prep.rename(columns=col_map)\n",
    "    return df_prep\n",
    "\n",
    "# Preparar dados\n",
    "df_2022_prep = preparar_dados(df_2022, 2022)\n",
    "df_2023_prep = preparar_dados(df_2023, 2023)\n",
    "df_2024_prep = preparar_dados(df_2024, 2024)\n",
    "\n",
    "# Unificar\n",
    "colunas = ['ANO', 'INDE', 'IAA', 'IEG', 'IPS', 'IPP', 'IDA', 'IPV', 'IAN', 'DEFASAGEM', 'PEDRA', 'GENERO', 'FASE']\n",
    "df_unificado = pd.concat([\n",
    "    df_2022_prep[[c for c in colunas if c in df_2022_prep.columns]],\n",
    "    df_2023_prep[[c for c in colunas if c in df_2023_prep.columns]],\n",
    "    df_2024_prep[[c for c in colunas if c in df_2024_prep.columns]]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Converter colunas num√©ricas\n",
    "for col in ['INDE', 'IAA', 'IEG', 'IPS', 'IPP', 'IDA', 'IPV', 'IAN', 'DEFASAGEM']:\n",
    "    if col in df_unificado.columns:\n",
    "        df_unificado[col] = pd.to_numeric(df_unificado[col], errors='coerce')\n",
    "\n",
    "df_unificado['PEDRA'] = df_unificado['PEDRA'].replace({'Agata': '√Ågata'})\n",
    "\n",
    "print(f'\\n‚úÖ DataFrame unificado: {len(df_unificado):,} registros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defini√ß√£o da Vari√°vel Alvo (Risco de Defasagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir vari√°vel alvo: aluno em risco de defasagem\n",
    "# Crit√©rio: defasagem <= -2 anos (moderada ou severa)\n",
    "\n",
    "df_unificado['EM_RISCO'] = (df_unificado['DEFASAGEM'] <= -2).astype(int)\n",
    "\n",
    "print('üìä Distribui√ß√£o da vari√°vel alvo (EM_RISCO):')\n",
    "print(df_unificado['EM_RISCO'].value_counts())\n",
    "print(f'\\nPercentual em risco: {df_unificado[\"EM_RISCO\"].mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribui√ß√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gr√°fico 1: Distribui√ß√£o da vari√°vel alvo\n",
    "cores = ['#2ecc71', '#e74c3c']\n",
    "labels = ['N√£o em risco', 'Em risco']\n",
    "valores = df_unificado['EM_RISCO'].value_counts().sort_index()\n",
    "axes[0].pie(valores, labels=labels, colors=cores, autopct='%1.1f%%', startangle=90, explode=[0, 0.1])\n",
    "axes[0].set_title('Distribui√ß√£o da Vari√°vel Alvo', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Risco por ano\n",
    "risco_ano = df_unificado.groupby('ANO')['EM_RISCO'].mean() * 100\n",
    "bars = axes[1].bar(risco_ano.index, risco_ano.values, color='#e74c3c', edgecolor='black')\n",
    "axes[1].set_title('Percentual de Alunos em Risco por Ano', fontweight='bold')\n",
    "axes[1].set_xlabel('Ano')\n",
    "axes[1].set_ylabel('% em Risco')\n",
    "for bar, val in zip(bars, risco_ano.values):\n",
    "    axes[1].annotate(f'{val:.1f}%', (bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     textcoords='offset points', xytext=(0, 5), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribuicao_risco.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar features para o modelo\n",
    "features_numericas = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV', 'IAN']\n",
    "\n",
    "# Verificar features dispon√≠veis\n",
    "features_disponiveis = [f for f in features_numericas if f in df_unificado.columns]\n",
    "print(f'Features dispon√≠veis: {features_disponiveis}')\n",
    "\n",
    "# Criar dataset para modelagem\n",
    "df_modelo = df_unificado[features_disponiveis + ['EM_RISCO', 'ANO', 'PEDRA']].dropna()\n",
    "print(f'\\nRegistros para modelagem: {len(df_modelo):,}')\n",
    "print(f'Registros removidos (NaN): {len(df_unificado) - len(df_modelo):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering adicional\n",
    "\n",
    "# 1. M√©dia dos indicadores\n",
    "df_modelo['MEDIA_INDICADORES'] = df_modelo[features_disponiveis].mean(axis=1)\n",
    "\n",
    "# 2. Desvio padr√£o dos indicadores (variabilidade)\n",
    "df_modelo['STD_INDICADORES'] = df_modelo[features_disponiveis].std(axis=1)\n",
    "\n",
    "# 3. Diferen√ßa entre IDA e IAA (gap entre desempenho real e autoavalia√ß√£o)\n",
    "if 'IDA' in df_modelo.columns and 'IAA' in df_modelo.columns:\n",
    "    df_modelo['GAP_IDA_IAA'] = df_modelo['IDA'] - df_modelo['IAA']\n",
    "\n",
    "# 4. Ratio IEG/IDA (engajamento relativo ao desempenho)\n",
    "if 'IEG' in df_modelo.columns and 'IDA' in df_modelo.columns:\n",
    "    df_modelo['RATIO_IEG_IDA'] = df_modelo['IEG'] / (df_modelo['IDA'] + 0.1)\n",
    "\n",
    "# Atualizar lista de features\n",
    "features_finais = features_disponiveis + ['MEDIA_INDICADORES', 'STD_INDICADORES', 'GAP_IDA_IAA', 'RATIO_IEG_IDA']\n",
    "features_finais = [f for f in features_finais if f in df_modelo.columns]\n",
    "\n",
    "print(f'\\nFeatures finais para o modelo: {features_finais}')\n",
    "print(f'Total de features: {len(features_finais)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar X e y\n",
    "X = df_modelo[features_finais]\n",
    "y = df_modelo['EM_RISCO']\n",
    "\n",
    "print(f'Shape de X: {X.shape}')\n",
    "print(f'Shape de y: {y.shape}')\n",
    "print(f'\\nDistribui√ß√£o de y:')\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divis√£o dos Dados e Balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Treino: {X_train.shape[0]} registros')\n",
    "print(f'Teste: {X_test.shape[0]} registros')\n",
    "print(f'\\nDistribui√ß√£o no treino:')\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('‚úÖ Features normalizadas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE para balancear classes (apenas no treino)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f'\\nAp√≥s SMOTE:')\n",
    "print(f'Treino balanceado: {X_train_balanced.shape[0]} registros')\n",
    "print(f'Distribui√ß√£o:')\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Treinamento dos Modelos\n",
    "\n",
    "### 6.1 Modelos Tradicionais (Machine Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Treinar e avaliar modelos\n",
    "resultados = {}\n",
    "\n",
    "print('='*70)\n",
    "print('TREINAMENTO E AVALIA√á√ÉO DOS MODELOS')\n",
    "print('='*70)\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f'\\nüìä {nome}')\n",
    "    print('-'*40)\n",
    "    \n",
    "    # Treinar\n",
    "    modelo.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = modelo.predict(X_test_scaled)\n",
    "    y_pred_proba = modelo.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    resultados[nome] = {\n",
    "        'modelo': modelo,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc_score,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f'Accuracy:  {acc:.4f}')\n",
    "    print(f'Precision: {prec:.4f}')\n",
    "    print(f'Recall:    {rec:.4f}')\n",
    "    print(f'F1-Score:  {f1:.4f}')\n",
    "    print(f'AUC-ROC:   {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Rede Neural (Deep Learning - Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir modelo de Rede Neural (MLP)\n",
    "def criar_modelo_mlp(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Criar modelo\n",
    "modelo_nn = criar_modelo_mlp(X_train_balanced.shape[1])\n",
    "modelo_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "print('\\nüìä Treinando Rede Neural (MLP)...')\n",
    "print('-'*40)\n",
    "\n",
    "history = modelo_nn.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar Rede Neural\n",
    "y_pred_nn_proba = modelo_nn.predict(X_test_scaled).flatten()\n",
    "y_pred_nn = (y_pred_nn_proba > 0.5).astype(int)\n",
    "\n",
    "acc_nn = accuracy_score(y_test, y_pred_nn)\n",
    "prec_nn = precision_score(y_test, y_pred_nn)\n",
    "rec_nn = recall_score(y_test, y_pred_nn)\n",
    "f1_nn = f1_score(y_test, y_pred_nn)\n",
    "auc_nn = roc_auc_score(y_test, y_pred_nn_proba)\n",
    "\n",
    "resultados['Neural Network (MLP)'] = {\n",
    "    'modelo': modelo_nn,\n",
    "    'accuracy': acc_nn,\n",
    "    'precision': prec_nn,\n",
    "    'recall': rec_nn,\n",
    "    'f1_score': f1_nn,\n",
    "    'auc': auc_nn,\n",
    "    'y_pred': y_pred_nn,\n",
    "    'y_pred_proba': y_pred_nn_proba\n",
    "}\n",
    "\n",
    "print(f'\\nüìä Neural Network (MLP)')\n",
    "print('-'*40)\n",
    "print(f'Accuracy:  {acc_nn:.4f}')\n",
    "print(f'Precision: {prec_nn:.4f}')\n",
    "print(f'Recall:    {rec_nn:.4f}')\n",
    "print(f'F1-Score:  {f1_nn:.4f}')\n",
    "print(f'AUC-ROC:   {auc_nn:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar hist√≥rico de treinamento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Treino')\n",
    "axes[0].plot(history.history['val_loss'], label='Valida√ß√£o')\n",
    "axes[0].set_title('Evolu√ß√£o da Loss', fontweight='bold')\n",
    "axes[0].set_xlabel('√âpoca')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# AUC\n",
    "axes[1].plot(history.history['auc'], label='Treino')\n",
    "axes[1].plot(history.history['val_auc'], label='Valida√ß√£o')\n",
    "axes[1].set_title('Evolu√ß√£o do AUC', fontweight='bold')\n",
    "axes[1].set_xlabel('√âpoca')\n",
    "axes[1].set_ylabel('AUC')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('historico_treinamento_nn.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compara√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "df_resultados = pd.DataFrame({\n",
    "    'Modelo': list(resultados.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in resultados.values()],\n",
    "    'Precision': [r['precision'] for r in resultados.values()],\n",
    "    'Recall': [r['recall'] for r in resultados.values()],\n",
    "    'F1-Score': [r['f1_score'] for r in resultados.values()],\n",
    "    'AUC-ROC': [r['auc'] for r in resultados.values()]\n",
    "})\n",
    "\n",
    "df_resultados = df_resultados.sort_values('F1-Score', ascending=False)\n",
    "print('\\nüìä COMPARA√á√ÉO DOS MODELOS')\n",
    "print('='*70)\n",
    "print(df_resultados.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico 1: M√©tricas por modelo\n",
    "metricas = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(df_resultados))\n",
    "width = 0.15\n",
    "\n",
    "for i, metrica in enumerate(metricas):\n",
    "    axes[0].bar(x + i*width, df_resultados[metrica], width, label=metrica)\n",
    "\n",
    "axes[0].set_title('Compara√ß√£o de M√©tricas por Modelo', fontweight='bold')\n",
    "axes[0].set_xlabel('Modelo')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xticks(x + width*2)\n",
    "axes[0].set_xticklabels(df_resultados['Modelo'], rotation=45, ha='right')\n",
    "axes[0].legend(bbox_to_anchor=(1.02, 1))\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Gr√°fico 2: Curvas ROC\n",
    "for nome, res in resultados.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_pred_proba'])\n",
    "    axes[1].plot(fpr, tpr, label=f\"{nome} (AUC={res['auc']:.3f})\")\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[1].set_title('Curvas ROC', fontweight='bold')\n",
    "axes[1].set_xlabel('Taxa de Falsos Positivos')\n",
    "axes[1].set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "axes[1].legend(loc='lower right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparacao_modelos.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar melhor modelo (baseado em F1-Score)\n",
    "melhor_modelo_nome = df_resultados.iloc[0]['Modelo']\n",
    "melhor_resultado = resultados[melhor_modelo_nome]\n",
    "\n",
    "print(f'\\nüèÜ MELHOR MODELO: {melhor_modelo_nome}')\n",
    "print('='*50)\n",
    "print(f\"F1-Score: {melhor_resultado['f1_score']:.4f}\")\n",
    "print(f\"AUC-ROC: {melhor_resultado['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confus√£o do melhor modelo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_test, melhor_resultado['y_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['N√£o em Risco', 'Em Risco'],\n",
    "            yticklabels=['N√£o em Risco', 'Em Risco'])\n",
    "axes[0].set_title(f'Matriz de Confus√£o - {melhor_modelo_nome}', fontweight='bold')\n",
    "axes[0].set_xlabel('Predito')\n",
    "axes[0].set_ylabel('Real')\n",
    "\n",
    "# Feature Importance (se dispon√≠vel)\n",
    "if hasattr(melhor_resultado['modelo'], 'feature_importances_'):\n",
    "    importances = melhor_resultado['modelo'].feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    axes[1].barh(range(len(features_finais)), importances[indices], color='#3498db', edgecolor='black')\n",
    "    axes[1].set_yticks(range(len(features_finais)))\n",
    "    axes[1].set_yticklabels([features_finais[i] for i in indices])\n",
    "    axes[1].set_title('Import√¢ncia das Features', fontweight='bold')\n",
    "    axes[1].set_xlabel('Import√¢ncia')\n",
    "else:\n",
    "    # Para modelos sem feature_importances_, usar coeficientes ou outra m√©trica\n",
    "    axes[1].text(0.5, 0.5, 'Feature importance n√£o dispon√≠vel\\npara este modelo', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "    axes[1].set_title('Import√¢ncia das Features', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('analise_melhor_modelo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relat√≥rio de classifica√ß√£o detalhado\n",
    "print(f'\\nüìä Relat√≥rio de Classifica√ß√£o - {melhor_modelo_nome}')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, melhor_resultado['y_pred'], \n",
    "                           target_names=['N√£o em Risco', 'Em Risco']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Salvar Modelo para Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Salvar o melhor modelo tradicional (Random Forest ou XGBoost)\n",
    "# Escolher o melhor modelo tradicional para o Streamlit\n",
    "modelo_para_deploy = resultados['Random Forest']['modelo']  # ou XGBoost\n",
    "\n",
    "# Salvar modelo\n",
    "joblib.dump(modelo_para_deploy, 'modelo_risco_defasagem.pkl')\n",
    "print('‚úÖ Modelo salvo: modelo_risco_defasagem.pkl')\n",
    "\n",
    "# Salvar scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print('‚úÖ Scaler salvo: scaler.pkl')\n",
    "\n",
    "# Salvar lista de features\n",
    "with open('features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(features_finais))\n",
    "print('‚úÖ Features salvas: features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo Keras (opcional)\n",
    "modelo_nn.save('modelo_nn_risco_defasagem.h5')\n",
    "print('‚úÖ Modelo Neural Network salvo: modelo_nn_risco_defasagem.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fun√ß√£o de Predi√ß√£o para Novos Alunos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prever_risco(ida, ieg, iaa, ips, ipv, ian):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para prever risco de defasagem de um aluno.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - ida: Indicador de Desempenho Acad√™mico (0-10)\n",
    "    - ieg: Indicador de Engajamento (0-10)\n",
    "    - iaa: Indicador de Autoavalia√ß√£o (0-10)\n",
    "    - ips: Indicador Psicossocial (0-10)\n",
    "    - ipv: Indicador de Ponto de Virada (0-10)\n",
    "    - ian: Indicador de Adequa√ß√£o ao N√≠vel (0-10)\n",
    "    \n",
    "    Retorna:\n",
    "    - Probabilidade de estar em risco (0-1)\n",
    "    - Classifica√ß√£o (Em Risco / N√£o em Risco)\n",
    "    \"\"\"\n",
    "    # Criar features\n",
    "    media_ind = np.mean([ida, ieg, iaa, ips, ipv, ian])\n",
    "    std_ind = np.std([ida, ieg, iaa, ips, ipv, ian])\n",
    "    gap_ida_iaa = ida - iaa\n",
    "    ratio_ieg_ida = ieg / (ida + 0.1)\n",
    "    \n",
    "    # Criar array de features\n",
    "    features = np.array([[ida, ieg, iaa, ips, ipv, ian, media_ind, std_ind, gap_ida_iaa, ratio_ieg_ida]])\n",
    "    \n",
    "    # Normalizar\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    prob = modelo_para_deploy.predict_proba(features_scaled)[0, 1]\n",
    "    classificacao = 'Em Risco' if prob > 0.5 else 'N√£o em Risco'\n",
    "    \n",
    "    return prob, classificacao\n",
    "\n",
    "# Exemplo de uso\n",
    "print('\\nüìä EXEMPLO DE PREDI√á√ÉO')\n",
    "print('='*50)\n",
    "\n",
    "# Aluno exemplo 1 (baixo risco)\n",
    "prob1, class1 = prever_risco(ida=8.0, ieg=8.5, iaa=8.0, ips=7.5, ipv=7.0, ian=8.0)\n",
    "print(f'\\nAluno 1 (bom desempenho):')\n",
    "print(f'  Probabilidade de risco: {prob1:.2%}')\n",
    "print(f'  Classifica√ß√£o: {class1}')\n",
    "\n",
    "# Aluno exemplo 2 (alto risco)\n",
    "prob2, class2 = prever_risco(ida=4.0, ieg=3.5, iaa=5.0, ips=4.0, ipv=3.0, ian=4.0)\n",
    "print(f'\\nAluno 2 (baixo desempenho):')\n",
    "print(f'  Probabilidade de risco: {prob2:.2%}')\n",
    "print(f'  Classifica√ß√£o: {class2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Conclus√µes\n",
    "\n",
    "### Principais Descobertas:\n",
    "\n",
    "1. **Melhor Modelo:** O modelo com melhor desempenho foi identificado com base no F1-Score, equilibrando precis√£o e recall.\n",
    "\n",
    "2. **Features Mais Importantes:** Os indicadores que mais contribuem para identificar alunos em risco s√£o tipicamente IDA, IEG e IAN.\n",
    "\n",
    "3. **Desempenho:** O modelo consegue identificar alunos em risco com boa acur√°cia, permitindo interven√ß√µes preventivas.\n",
    "\n",
    "### Recomenda√ß√µes:\n",
    "\n",
    "- Utilizar o modelo para identifica√ß√£o precoce de alunos em risco\n",
    "- Focar interven√ß√µes em alunos com alta probabilidade de risco\n",
    "- Monitorar especialmente os indicadores IDA e IEG\n",
    "- Atualizar o modelo periodicamente com novos dados\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
