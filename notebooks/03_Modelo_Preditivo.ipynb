{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Datathon FIAP - Passos M√°gicos\n",
    "## Modelo Preditivo de Risco de Defasagem\n",
    "\n",
    "Este notebook desenvolve um modelo de Machine Learning para identificar alunos em risco de defasagem educacional, utilizando os indicadores do PEDE.\n",
    "\n",
    "**Objetivo:** Criar um modelo preditivo que identifique padr√µes nos indicadores de desempenho (IDA, IEG, IAA, IPS, IPV, INDE) que permitam prever alunos em risco de defasagem escolar.\n",
    "\n",
    "**Classifica√ß√£o de Risco (Metodologia Passos M√°gicos):**\n",
    "- **Sem Risco**: Aluno em fase adequada ou adiantado (D ‚â• 0)\n",
    "- **Risco Moderado**: Aluno 1-2 fases atrasado (0 > D ‚â• -2)\n",
    "- **Risco Severo**: Aluno 3+ fases atrasado (D < -2)\n",
    "\n",
    "**Autor:** Leandro Leme Crespo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar o reposit√≥rio do GitHub (executar apenas no Google Colab)\n",
    "!git clone https://github.com/LeandroCrespo/datathon-passos-magicos.git\n",
    "print('‚úÖ Reposit√≥rio clonado com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar bibliotecas necess√°rias\n",
    "!pip install openpyxl scikit-learn imbalanced-learn -q\n",
    "print('‚úÖ Bibliotecas instaladas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úÖ Bibliotecas importadas com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados (do reposit√≥rio GitHub)\n",
    "CAMINHO_ARQUIVO = '/content/datathon-passos-magicos/data/BASE_DE_DADOS_PEDE_2024_DATATHON.xlsx'\n",
    "\n",
    "# Carregar a primeira planilha (dados de 2024)\n",
    "df = pd.read_excel(CAMINHO_ARQUIVO)\n",
    "\n",
    "print(f'üìä Dados carregados: {df.shape[0]:,} alunos, {df.shape[1]} colunas')\n",
    "print(f'\\nColunas dispon√≠veis:')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear colunas para padronizar\n",
    "col_map = {}\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if col_lower == 'iaa': col_map[col] = 'IAA'\n",
    "    elif col_lower == 'ieg' and 'destaque' not in col_lower: col_map[col] = 'IEG'\n",
    "    elif col_lower == 'ips': col_map[col] = 'IPS'\n",
    "    elif col_lower == 'ida' and 'destaque' not in col_lower: col_map[col] = 'IDA'\n",
    "    elif col_lower == 'ipv' and 'destaque' not in col_lower: col_map[col] = 'IPV'\n",
    "    elif col_lower == 'ian': col_map[col] = 'IAN'\n",
    "    elif 'defas' in col_lower: col_map[col] = 'DEFASAGEM'\n",
    "\n",
    "# Mapear INDE\n",
    "if 'INDE 22' in df.columns:\n",
    "    col_map['INDE 22'] = 'INDE'\n",
    "\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# Converter para num√©rico\n",
    "indicadores = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV', 'IAN', 'INDE', 'DEFASAGEM']\n",
    "for col in indicadores:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print('‚úÖ Colunas renomeadas e convertidas')\n",
    "print(f'\\nIndicadores dispon√≠veis: {[c for c in indicadores if c in df.columns]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defini√ß√£o da Vari√°vel Alvo (Classifica√ß√£o de Risco)\n",
    "\n",
    "Seguindo a metodologia oficial da Passos M√°gicos:\n",
    "\n",
    "**D = Fase Efetiva - Fase Ideal**\n",
    "\n",
    "| Defasagem (D) | Classifica√ß√£o | Classe |\n",
    "|---------------|---------------|--------|\n",
    "| D ‚â• 0 | Sem Risco (Em fase) | 0 |\n",
    "| 0 > D ‚â• -2 | Risco Moderado | 1 |\n",
    "| D < -2 | Risco Severo | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar vari√°vel alvo com 3 classes\n",
    "def classificar_risco(d):\n",
    "    \"\"\"Classifica o risco de defasagem conforme metodologia Passos M√°gicos\"\"\"\n",
    "    if pd.isna(d):\n",
    "        return None\n",
    "    if d >= 0:\n",
    "        return 0  # Sem Risco (Em fase ou adiantado)\n",
    "    elif d >= -2:\n",
    "        return 1  # Risco Moderado (1-2 fases atrasado)\n",
    "    else:\n",
    "        return 2  # Risco Severo (3+ fases atrasado)\n",
    "\n",
    "df['CLASSE_RISCO'] = df['DEFASAGEM'].apply(classificar_risco)\n",
    "\n",
    "# Remover registros sem classifica√ß√£o\n",
    "df = df.dropna(subset=['CLASSE_RISCO'])\n",
    "df['CLASSE_RISCO'] = df['CLASSE_RISCO'].astype(int)\n",
    "\n",
    "print('üìä Distribui√ß√£o da vari√°vel alvo (CLASSE_RISCO):')\n",
    "print(df['CLASSE_RISCO'].value_counts().sort_index())\n",
    "print(f'\\nPercentuais:')\n",
    "print(f'   Sem Risco (0): {(df[\"CLASSE_RISCO\"] == 0).mean()*100:.1f}%')\n",
    "print(f'   Risco Moderado (1): {(df[\"CLASSE_RISCO\"] == 1).mean()*100:.1f}%')\n",
    "print(f'   Risco Severo (2): {(df[\"CLASSE_RISCO\"] == 2).mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribui√ß√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "cores = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "labels = ['Sem Risco', 'Risco Moderado', 'Risco Severo']\n",
    "valores = df['CLASSE_RISCO'].value_counts().sort_index()\n",
    "\n",
    "bars = axes[0].bar(labels, valores.values, color=cores, edgecolor='black')\n",
    "axes[0].set_title('Distribui√ß√£o das Classes de Risco', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Quantidade de Alunos')\n",
    "for bar, val in zip(bars, valores.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                 f'{val}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "axes[1].pie(valores.values, labels=labels, colors=cores, autopct='%1.1f%%',\n",
    "            startangle=90, explode=(0, 0.05, 0.1))\n",
    "axes[1].set_title('Propor√ß√£o das Classes de Risco', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribuicao_risco.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise Explorat√≥ria dos Indicadores por Classe de Risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©dia dos indicadores por classe de risco\n",
    "# IMPORTANTE: N√£o usamos IAN pois √© derivado da defasagem\n",
    "indicadores_modelo = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV', 'INDE']\n",
    "indicadores_existentes = [i for i in indicadores_modelo if i in df.columns]\n",
    "\n",
    "media_por_classe = df.groupby('CLASSE_RISCO')[indicadores_existentes].mean()\n",
    "media_por_classe.index = ['Sem Risco', 'Risco Moderado', 'Risco Severo']\n",
    "\n",
    "print('üìä M√©dia dos Indicadores por Classe de Risco:')\n",
    "print(media_por_classe.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(indicadores_existentes))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, media_por_classe.loc['Sem Risco'], width, label='Sem Risco', color='#2ecc71')\n",
    "bars2 = ax.bar(x, media_por_classe.loc['Risco Moderado'], width, label='Risco Moderado', color='#f39c12')\n",
    "bars3 = ax.bar(x + width, media_por_classe.loc['Risco Severo'], width, label='Risco Severo', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Indicador')\n",
    "ax.set_ylabel('M√©dia')\n",
    "ax.set_title('Compara√ß√£o dos Indicadores por Classe de Risco', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(indicadores_existentes)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparacao_indicadores.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o dos Dados para o Modelo\n",
    "\n",
    "**Features utilizadas:** IDA, IEG, IAA, IPS, IPV, INDE\n",
    "\n",
    "**IMPORTANTE:** N√£o utilizamos o IAN como feature porque ele √© derivado diretamente da defasagem (vari√°vel alvo). Usar o IAN seria redundante e n√£o permitiria identificar padr√µes nos outros indicadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features (SEM IAN)\n",
    "features = ['IDA', 'IEG', 'IAA', 'IPS', 'IPV', 'INDE']\n",
    "features_existentes = [f for f in features if f in df.columns]\n",
    "\n",
    "print(f'üìã Features utilizadas: {features_existentes}')\n",
    "print(f'\\n‚ö†Ô∏è IAN n√£o √© utilizado pois √© derivado da defasagem (vari√°vel alvo)')\n",
    "\n",
    "# Preparar dados\n",
    "df_modelo = df[features_existentes + ['CLASSE_RISCO']].dropna()\n",
    "print(f'\\nüìä Registros para modelagem: {len(df_modelo)}')\n",
    "\n",
    "X = df_modelo[features_existentes]\n",
    "y = df_modelo['CLASSE_RISCO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e teste (estratificado)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'üìä Divis√£o dos dados:')\n",
    "print(f'   Treino: {len(X_train):,} registros')\n",
    "print(f'   Teste: {len(X_test):,} registros')\n",
    "print(f'\\nüìä Distribui√ß√£o no treino:')\n",
    "print(f'   Sem Risco: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)')\n",
    "print(f'   Risco Moderado: {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)')\n",
    "print(f'   Risco Severo: {(y_train == 2).sum():,} ({(y_train == 2).mean()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('‚úÖ Features normalizadas com StandardScaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balanceamento de Classes com SMOTE\n",
    "\n",
    "Como temos um desbalanceamento significativo entre as classes (poucos alunos em risco severo), utilizamos a t√©cnica **SMOTE** (Synthetic Minority Over-sampling Technique) para criar exemplos sint√©ticos das classes minorit√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print('üìä Balanceamento com SMOTE:')\n",
    "print(f'   Antes: {dict(y_train.value_counts().sort_index())}')\n",
    "print(f'   Depois: {dict(pd.Series(y_train_smote).value_counts().sort_index())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo Random Forest\n",
    "modelo = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "modelo.fit(X_train_smote, y_train_smote)\n",
    "print('‚úÖ Modelo Random Forest treinado com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avalia√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer predi√ß√µes\n",
    "y_pred = modelo.predict(X_test_scaled)\n",
    "y_pred_proba = modelo.predict_proba(X_test_scaled)\n",
    "\n",
    "# M√©tricas gerais\n",
    "print('='*60)\n",
    "print('üìä RESULTADOS DO MODELO')\n",
    "print('='*60)\n",
    "print(f'\\nüéØ Acur√°cia: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "\n",
    "print('\\nüìã Relat√≥rio de Classifica√ß√£o:')\n",
    "print(classification_report(y_test, y_pred, \n",
    "                           target_names=['Sem Risco', 'Risco Moderado', 'Risco Severo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Sem Risco', 'Risco Moderado', 'Risco Severo'],\n",
    "            yticklabels=['Sem Risco', 'Risco Moderado', 'Risco Severo'])\n",
    "ax.set_xlabel('Predito')\n",
    "ax.set_ylabel('Real')\n",
    "ax.set_title('Matriz de Confus√£o', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('matriz_confusao.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nüìä Interpreta√ß√£o da Matriz de Confus√£o:')\n",
    "print(f'   Sem Risco corretamente identificados: {cm[0,0]}')\n",
    "print(f'   Risco Moderado corretamente identificados: {cm[1,1]}')\n",
    "print(f'   Risco Severo corretamente identificados: {cm[2,2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular import√¢ncia das features\n",
    "importancias = pd.DataFrame({\n",
    "    'Feature': features_existentes,\n",
    "    'Import√¢ncia': modelo.feature_importances_\n",
    "}).sort_values('Import√¢ncia', ascending=False)\n",
    "\n",
    "print('üìä Import√¢ncia das Features:')\n",
    "for _, row in importancias.iterrows():\n",
    "    print(f'   {row[\"Feature\"]}: {row[\"Import√¢ncia\"]*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar import√¢ncia\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cores = plt.cm.Blues(np.linspace(0.4, 0.8, len(importancias)))\n",
    "bars = ax.barh(importancias['Feature'], importancias['Import√¢ncia'], color=cores)\n",
    "\n",
    "ax.set_xlabel('Import√¢ncia')\n",
    "ax.set_title('Import√¢ncia das Features no Modelo', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, val in zip(bars, importancias['Import√¢ncia']):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val*100:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Valida√ß√£o Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√£o cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(modelo, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "print('üìä Valida√ß√£o Cruzada (5-fold):')\n",
    "print(f'   Scores: {scores}')\n",
    "print(f'   M√©dia: {scores.mean()*100:.2f}%')\n",
    "print(f'   Desvio Padr√£o: {scores.std()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo e artefatos\n",
    "output_dir = '/content/datathon-passos-magicos/streamlit/'\n",
    "\n",
    "# Salvar modelo\n",
    "with open(f'{output_dir}modelo_risco_defasagem.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo, f)\n",
    "print(f'‚úÖ Modelo salvo em {output_dir}modelo_risco_defasagem.pkl')\n",
    "\n",
    "# Salvar scaler\n",
    "with open(f'{output_dir}scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f'‚úÖ Scaler salvo em {output_dir}scaler.pkl')\n",
    "\n",
    "# Salvar lista de features\n",
    "with open(f'{output_dir}features.txt', 'w') as f:\n",
    "    f.write(','.join(features_existentes))\n",
    "print(f'‚úÖ Features salvas em {output_dir}features.txt')\n",
    "\n",
    "# Salvar info do modelo\n",
    "modelo_info = {\n",
    "    'features': features_existentes,\n",
    "    'classes': {0: 'Sem Risco', 1: 'Risco Moderado', 2: 'Risco Severo'},\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'feature_importance': dict(zip(features_existentes, modelo.feature_importances_.tolist()))\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}modelo_info.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo_info, f)\n",
    "print(f'‚úÖ Info do modelo salva em {output_dir}modelo_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exemplo de Predi√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de predi√ß√£o para cada classe\n",
    "print('='*60)\n",
    "print('üìä EXEMPLOS DE PREDI√á√ÉO')\n",
    "print('='*60)\n",
    "\n",
    "for classe in [0, 1, 2]:\n",
    "    exemplo = df_modelo[df_modelo['CLASSE_RISCO'] == classe].iloc[0]\n",
    "    X_exemplo = scaler.transform([exemplo[features_existentes].values])\n",
    "    pred = modelo.predict(X_exemplo)[0]\n",
    "    proba = modelo.predict_proba(X_exemplo)[0]\n",
    "    \n",
    "    classe_nome = ['Sem Risco', 'Risco Moderado', 'Risco Severo'][classe]\n",
    "    pred_nome = ['Sem Risco', 'Risco Moderado', 'Risco Severo'][pred]\n",
    "    \n",
    "    print(f'\\nüéØ Exemplo de aluno \"{classe_nome}\":')\n",
    "    print(f'   Indicadores: {dict(exemplo[features_existentes].round(2))}')\n",
    "    print(f'   Predi√ß√£o: {pred_nome}')\n",
    "    print(f'   Probabilidades: Sem Risco={proba[0]:.2f}, Moderado={proba[1]:.2f}, Severo={proba[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclus√µes\n",
    "\n",
    "### Resultados do Modelo\n",
    "\n",
    "O modelo Random Forest foi treinado para classificar alunos em tr√™s categorias de risco de defasagem:\n",
    "\n",
    "| M√©trica | Valor |\n",
    "|---------|-------|\n",
    "| **Acur√°cia Geral** | ~75% |\n",
    "| **Recall (Sem Risco)** | ~69% |\n",
    "| **Recall (Risco Moderado)** | ~82% |\n",
    "| **Recall (Risco Severo)** | Baixo (poucos exemplos) |\n",
    "\n",
    "### Import√¢ncia das Features\n",
    "\n",
    "Os indicadores mais importantes para prever o risco de defasagem s√£o:\n",
    "\n",
    "1. **INDE** (~35%) - √çndice geral de desenvolvimento\n",
    "2. **IPV** (~16%) - Ponto de Virada\n",
    "3. **IDA** (~15%) - Desempenho Acad√™mico\n",
    "4. **IAA** (~13%) - Autoavalia√ß√£o\n",
    "5. **IEG** (~12%) - Engajamento\n",
    "6. **IPS** (~10%) - Psicossocial\n",
    "\n",
    "### Utilidade Pr√°tica\n",
    "\n",
    "O modelo permite:\n",
    "- Identificar **padr√µes nos indicadores** associados a alunos em defasagem\n",
    "- Alertar sobre alunos que **hoje est√£o bem**, mas t√™m padr√µes similares aos que est√£o em defasagem\n",
    "- Permitir **interven√ß√£o preventiva** antes que a defasagem aconte√ßa\n",
    "\n",
    "### Limita√ß√µes\n",
    "\n",
    "- Poucos exemplos de Risco Severo (3.3% dos dados)\n",
    "- Modelo treinado apenas com dados de 2024\n",
    "- Recomenda-se usar como apoio √† decis√£o, n√£o como √∫nica fonte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
